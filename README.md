# ЁЯУД PDF Question Answering in Bangla

Welcome!  
This project demonstrates how to convert Bangla PDF documents into a searchable knowledge base that supports automatic question answering using OCR, NLP preprocessing, semantic embedding, and local language models.

---

## ЁЯЪА Features

- тЬЕ Convert scanned or digital PDFs to Bangla text using OCR
- тЬЕ Clean, normalize, and chunk content intelligently
- тЬЕ Generate semantic vector embeddings for each chunk
- тЬЕ Store chunks using FAISS for fast similarity search
- тЬЕ Ask Bangla questions and receive contextual answers in real-time

---

## ЁЯУБ Project Structure

project/  
тФЬтФАтФА data/ # PNG images (1 per page) exported from PDF  
тФЬтФАтФА output/ # All generated files (text, chunks, embeddings)  
тФВ тФЬтФАтФА extracted.txt  
тФВ тФЬтФАтФА cleaned.md  
тФВ тФЬтФАтФА normalized.txt  
тФВ тФЬтФАтФА chunks.txt  
тФВ тФФтФАтФА answer.txt  
тФЬтФАтФА steps/ # Modular steps (OCR, clean, normalize, embed, QA)  
тФЬтФАтФА main.py # Main pipeline script  
тФФтФАтФА requirements.txt



---

## ЁЯЫая╕П Setup Guide

### 1я╕ПтГг Convert PDF to Images
Export your PDF pages as `.png` images (e.g. using Adobe Acrobat or other tools) and save them inside the `data/` folder.

### 2я╕ПтГг Set up Python Environment
```bash
python -m venv venv
# Activate on Windows
venv\Scripts\activate
# Or on Linux/macOS
source venv/bin/activate
```

### 3я╕ПтГг Install Requirements

`pip install -r requirements.txt`

### 4я╕ПтГг Run the Pipeline

`python main.py`


## ЁЯУд Output

The following files will be generated in the `output/` folder:

-   `extracted.txt` тАУ Raw OCR output
    
-   `cleaned.md` тАУ Cleaned and markdown-formatted content
    
-   `normalized.txt` тАУ Normalized Bangla text
    
-   `chunks.txt` тАУ Text chunks used for retrieval
    
-   `answer.txt` тАУ Final answer generated based on your query

## ЁЯТм Sample Queries

### Bangla

ржкрзНрж░рж╢рзНржи: ржЕржирзБржкржорзЗрж░ ржнрж╛рж╖рж╛ржпрж╝ рж╕рзБржкрзБрж░рзБрж╖ ржХрж╛ржХрзЗ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ?
ржЙрждрзНрждрж░: ржХрзА

ржкрзНрж░рж╢рзНржи: ржХрж╛ржХрзЗ ржЕржирзБржкржорзЗрж░ ржнрж╛ржЧрзНржп ржжрзЗржмрждрж╛ ржмрж▓рзЗ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ?
ржЙрждрзНрждрж░: ржЕржкрж░рж┐ржЪрж┐рждрж╛

ржкрзНрж░рж╢рзНржи: ржмрж┐ржпрж╝рзЗрж░ рж╕ржоржпрж╝ ржХрж▓рзНржпрж╛ржгрзАрж░ ржкрзНрж░ржХрзГржд ржмржпрж╝рж╕ ржХржд ржЫрж┐рж▓?
ржЙрждрзНрждрж░: рзирзо ржмржЫрж░

ржкрзНрж░рж╢рзНржи: ржмрж┐ржжрзНрж░рзЛрж╣рзА ржХржмрж┐рждрж╛рж░ ржорзВрж▓ ржмрж╛рж░рзНрждрж╛ ржХрзА?
ржЙрждрзНрждрж░: ржорзАржорж╛ржВрж╕рж╛

ржкрзНрж░рж╢рзНржи: ржЕржирзБржкржо ржЪрж░рж┐рждрзНрж░рзЗрж░ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп ржХрзА?
ржЙрждрзНрждрж░: ржмрзИрж╕рж╛ржжрзГрж╢рзНржпржкрзВрж░рзНржг

ржкрзНрж░рж╢рзНржи: ржХрж▓рзНржпрж╛ржгрзА ржЪрж░рж┐рждрзНрж░ржЯрж┐ ржХрзА ржЙржкрж╕рзНржерж╛ржкржи ржХрж░рзЗ?
ржЙрждрзНрждрж░: рж╣рж╛рж░рзБржи ржорж┐ржпрж╝рж╛

ржкрзНрж░рж╢рзНржи: ржЕржкрж░рж┐ржЪрж┐рждрж╛ ржЧрж▓рзНржкрзЗрж░ ржкржЯржнрзВржорж┐ ржХрзА?
ржЙрждрзНрждрж░: ржЬрзЛржбрж╝рж╛рж░рзНрж╕рж╛ржХрзЛрж░ ржарж╛ржХрзБрж░ ржмрж╛ржбрж╝рж┐рждрзЗ

ржкрзНрж░рж╢рзНржи: ржкржгржкрзНрж░ржерж╛рж░ ржХрзБржкрзНрж░ржнрж╛ржм ржХрзАржнрж╛ржмрзЗ ржжрзЗржЦрж╛ржирзЛ рж╣ржпрж╝рзЗржЫрзЗ?
ржЙрждрзНрждрж░: ржзрж░рзНржоржирж┐рж╖рзНржарж╛


### ЁЯз░ Tools & Libraries Used

| Tool / Library       | Purpose / Role                                  |
|----------------------|--------------------------------------------------|
| Python               | Core programming language                       |
| PaddleOCR            | OCR engine with Bengali language support        |
| SentenceTransformers | Generating semantic embeddings from text        |
| FAISS                | Vector similarity search                        |
| Hugging Face         | Pretrained models and transformers              |
| PyMuPDF / PIL        | PDF and image handling                          |
| BanglaBERT           | Bengali language understanding & QA             |


## ЁЯза Design & Methodology Q&A

### ЁЯЕа What method or library did you use to extract the text, and why?

I have used **PaddleOCR** with Bangla language support because of its strong multilingual capabilities and good accuracy on printed text.  
ЁЯУМ Challenge: The PDF was converted to images, and OCR sometimes produced broken words, which were handled via post-cleaning.

----------

### ЁЯЕа What chunking strategy did you use?

I have used a **sentence-based chunking** approach, grouping 2тАУ3 Bangla sentences per chunk.  
тЬЕ Sentence chunking works well for semantic retrieval because:

-   It preserves topic continuity
    
-   Avoids arbitrary cuts
    
-   Helps align with user questions that typically span sentence-level logic
    
----------

### ЁЯЕа What embedding model did you use? Why?

I have used a **multilingual MiniLM-based SentenceTransformer** model:  
`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`  
тЬЕ It supports Bangla, produces fast dense embeddings, and works well for low-resource languages.

----------

### ЁЯЕа How is the query compared with the document chunks?

I have used **cosine similarity** between the embedding of the user query and each chunk vector stored in **FAISS**.  
тЬЕ FAISS is chosen for speed and scalability, even with thousands of chunks.

----------

### ЁЯЕа How do you ensure meaningful comparisons?

-   Used the same embedding model for both query and chunks
    
-   All text is normalized and preprocessed before vectorization
    
-   Selected **top-k similar chunks** to build the prompt context
    

ЁЯУЙ If the query is vague or missing context, results may be off тАФ better prompts and more contextual chunking can help.

----------

### ЁЯЕа Do the results seem relevant?

Partially тАФ depending on the OCR quality and model capabilities.  
тЭМ Sometimes the QA model returns incorrect or incomplete answers.  
ЁЯЪз Future Improvements:

-   Use larger, instruction-tuned models (e.g. LLaMA-based)
    
-   Fine-tune chunking and QA prompt construction
    
-   Better OCR post-processing to improve text quality